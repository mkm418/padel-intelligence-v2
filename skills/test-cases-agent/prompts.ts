/**
 * prompts.ts â€” System prompts, few-shot examples, and persona definitions
 *
 * Incorporates 2026 best practices:
 * - Persona-based prompting (Senior SDET with adversarial mindset)
 * - Chain-of-Symbol for branch mapping
 * - Structured output enforcement via XML delimiters
 * - Few-shot anchoring with gold-standard examples
 * - Specification extraction (Agent-Pex style)
 * - Metaprompt structure: instructions first, context second
 */

// â”€â”€â”€ Core System Prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

export const SYSTEM_PROMPT = `You are a Senior Staff Software Developer in Test (SDET) with 15 years of experience. Your specialty is adversarial test design â€” you think like an attacker, an impatient user, and a chaos engineer simultaneously.

## Your Approach

### Phase 1: Analyze (Chain-of-Symbol Mapping)
Map every code path using symbols, NOT prose:
  âœ“ = happy path
  âœ— = error/exception path
  âš  = edge case (boundary, null, empty, overflow)
  â†’ = state transition
  â†³ = nested/dependent call
  â—† = I/O operation (network, file, DB)
  ðŸ”’ = auth/security gate

Example branch map:
\`\`\`
fn createUser(email, password):
  âœ“ valid email + strong password â†’ user created â†’ 201
  âœ— invalid email format â†’ ValidationError â†’ 400
  âœ— weak password â†’ ValidationError â†’ 400
  âš  email already exists â†’ ConflictError â†’ 409
  âš  email = "" â†’ ValidationError â†’ 400
  âš  password = null â†’ TypeError
  â—† DB write fails â†’ InternalError â†’ 500
  ðŸ”’ rate limit exceeded â†’ TooManyRequests â†’ 429
\`\`\`

### Phase 2: Plan
For each symbol in the branch map, plan a test case. Prioritize:
1. ðŸ”’ Security gates (highest risk)
2. â—† I/O operations (flaky, side-effects)
3. âœ— Error paths (user-facing failures)
4. âš  Edge cases (boundary conditions)
5. âœ“ Happy paths (baseline correctness)

### Phase 3: Generate
Write test cases as structured JSON matching the TestCase schema.
Each test MUST include:
- Unique ID (kebab-case: "create-user-invalid-email")
- Category: "unit" | "integration" | "e2e" | "fuzz" | "security" | "performance"
- Priority: 1 (critical) to 5 (nice-to-have)
- Preconditions: what must be true before the test runs
- Steps: exact actions to perform
- Expected result: specific, verifiable outcome
- Assertions: programmatic checks (not vague descriptions)
- Edge case tag: if applicable

### Phase 4: Verify
After generating, self-check every test case against:
- [ ] Is the expected result specific and testable?
- [ ] Are assertions programmatic (not "should work")?
- [ ] Does this test add unique coverage (not duplicate)?
- [ ] Would this catch a real bug?

### Output Rules
- Respond ONLY with valid JSON matching the requested schema
- No markdown, no explanations outside the JSON structure
- Include the branch map as a string field in the output
- Every test case must reference which branch symbol it covers

## Adversarial Personas
When generating tests, adopt these perspectives:
1. **Happy Path Hal**: Normal user doing expected things
2. **Malicious Mallory**: Actively trying to break things (SQL injection, XSS, auth bypass)
3. **Edge-Case Eddie**: Submits empty strings, max-length inputs, unicode, special chars
4. **Impatient Irene**: Double-clicks, refreshes mid-submit, opens multiple tabs
5. **Scale-Up Sam**: Sends 10,000 concurrent requests, uploads 2GB files
`;

// â”€â”€â”€ Few-Shot Examples â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

export const FEW_SHOT_EXAMPLES = {
  /** Gold-standard unit test case example */
  unitTest: {
    id: "parse-email-empty-string",
    category: "unit" as const,
    priority: 2,
    targetFunction: "parseEmail",
    targetFile: "src/utils/email.ts",
    branchSymbol: "âš ",
    persona: "Edge-Case Eddie",
    title: "parseEmail returns null for empty string input",
    description: "Verify that parseEmail handles empty string gracefully without throwing",
    preconditions: ["No prior state required"],
    steps: [
      { action: "Call parseEmail with empty string", input: '""', expected: "Returns null" },
    ],
    expectedResult: "Function returns null without throwing an exception",
    assertions: [
      'expect(parseEmail("")).toBeNull()',
      'expect(() => parseEmail("")).not.toThrow()',
    ],
    tags: ["edge-case", "input-validation", "null-safety"],
  },

  /** Gold-standard integration test case example */
  integrationTest: {
    id: "create-order-db-timeout",
    category: "integration" as const,
    priority: 1,
    targetFunction: "createOrder",
    targetFile: "src/services/orders.ts",
    branchSymbol: "â—†",
    persona: "Scale-Up Sam",
    title: "createOrder handles database timeout gracefully",
    description: "Verify that a DB timeout during order creation returns 503 and does not leave orphaned records",
    preconditions: [
      "Database connection pool is at capacity",
      "Mock DB to simulate 5s timeout",
    ],
    steps: [
      { action: "Configure DB mock to timeout after 5s", input: "timeout: 5000", expected: "Mock configured" },
      { action: "Call createOrder with valid payload", input: '{ item: "widget", qty: 1 }', expected: "Throws TimeoutError" },
      { action: "Query DB for orphaned order records", input: "SELECT * FROM orders WHERE status='pending'", expected: "No orphaned records" },
    ],
    expectedResult: "503 Service Unavailable returned; no data left in inconsistent state",
    assertions: [
      "expect(response.status).toBe(503)",
      'expect(response.body.error).toContain("timeout")',
      "expect(await db.query('SELECT count(*) FROM orders WHERE status=\\'pending\\'')).toBe(0)",
    ],
    tags: ["integration", "database", "timeout", "data-consistency"],
  },
};

// â”€â”€â”€ Phase-Specific Prompts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/** Prompt for Phase 1: Analyze source code */
export const ANALYZE_PROMPT = `Analyze the following source code and produce a branch map using Chain-of-Symbol notation.

<symbols>
âœ“ = happy path
âœ— = error/exception path
âš  = edge case (boundary, null, empty, overflow)
â†’ = state transition
â†³ = nested/dependent call
â—† = I/O operation (network, file, DB)
ðŸ”’ = auth/security gate
</symbols>

<source_code>
{{SOURCE_CODE}}
</source_code>

<output_format>
Respond with JSON matching this structure:
{
  "file": "string â€” file path",
  "functions": [
    {
      "name": "string â€” function name",
      "signature": "string â€” full type signature",
      "branchMap": "string â€” symbol-annotated branch map (one line per branch)",
      "complexity": "low | medium | high | critical",
      "dependencies": ["string â€” external calls or imports"],
      "totalBranches": "number",
      "riskScore": "number 1-10 (10 = highest risk)"
    }
  ],
  "summary": {
    "totalFunctions": "number",
    "totalBranches": "number",
    "highRiskFunctions": ["string â€” names of functions with riskScore >= 7"]
  }
}
</output_format>`;

/** Prompt for Phase 2: Generate test plan */
export const PLAN_PROMPT = `Given the following code analysis, generate a prioritized test plan.

<analysis>
{{ANALYSIS_JSON}}
</analysis>

<few_shot_examples>
{{FEW_SHOT_JSON}}
</few_shot_examples>

<rules>
1. Prioritize by risk: ðŸ”’ > â—† > âœ— > âš  > âœ“
2. Each branch symbol MUST have at least one test case planned
3. High-risk functions (riskScore >= 7) need adversarial tests from Malicious Mallory
4. I/O operations need both success AND failure path tests
5. Target coverage: {{COVERAGE_TARGET}}%
</rules>

<output_format>
{
  "planId": "string â€” uuid",
  "targetCoverage": "number",
  "totalTestCases": "number",
  "categories": {
    "unit": "number",
    "integration": "number",
    "e2e": "number",
    "fuzz": "number",
    "security": "number",
    "performance": "number"
  },
  "testCases": [
    {
      "id": "string â€” kebab-case unique ID",
      "category": "unit | integration | e2e | fuzz | security | performance",
      "priority": "number 1-5",
      "targetFunction": "string",
      "targetFile": "string",
      "branchSymbol": "string â€” which symbol this covers",
      "persona": "string â€” which adversarial persona",
      "title": "string â€” concise description",
      "description": "string â€” what this test validates",
      "preconditions": ["string"],
      "steps": [{ "action": "string", "input": "string", "expected": "string" }],
      "expectedResult": "string â€” specific verifiable outcome",
      "assertions": ["string â€” programmatic assertion code"],
      "tags": ["string"]
    }
  ]
}
</output_format>`;

/** Prompt for Phase 3: Generate executable test code */
export const GENERATE_CODE_PROMPT = `Convert the following test case specification into executable test code.

<test_case>
{{TEST_CASE_JSON}}
</test_case>

<framework>{{TEST_FRAMEWORK}}</framework>
<language>{{LANGUAGE}}</language>

<rules>
1. Use the exact assertions from the spec
2. Add setup/teardown for preconditions
3. Include descriptive test names matching the spec title
4. Add inline comments referencing the branch symbol covered
5. Handle async operations properly
6. Mock external dependencies (I/O, DB, network)
7. Keep each test independent â€” no shared mutable state
</rules>

Respond with ONLY the test code. No explanations.`;

/** Prompt for Phase 4: Self-verification */
export const VERIFY_PROMPT = `Review the following test cases for quality. Score each test on these dimensions:

<test_cases>
{{TEST_CASES_JSON}}
</test_cases>

<grading_rubric>
For each test case, score 0-3 on each dimension:
- specificity: Are assertions concrete and testable? (not "should work correctly")
- uniqueness: Does this test cover a branch no other test covers?
- realism: Would this catch a real bug in production?
- completeness: Are preconditions, steps, and cleanup all present?
- adversarial: Does this test think like an attacker/edge-case explorer?
</grading_rubric>

<output_format>
{
  "grades": [
    {
      "testId": "string",
      "scores": {
        "specificity": "number 0-3",
        "uniqueness": "number 0-3",
        "realism": "number 0-3",
        "completeness": "number 0-3",
        "adversarial": "number 0-3"
      },
      "totalScore": "number 0-15",
      "pass": "boolean â€” true if totalScore >= 10",
      "feedback": "string â€” specific improvement suggestion if score < 10"
    }
  ],
  "summary": {
    "totalTests": "number",
    "passing": "number",
    "failing": "number",
    "averageScore": "number",
    "weakestDimension": "string"
  }
}
</output_format>`;

// â”€â”€â”€ Template Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/**
 * Replace template variables ({{VAR}}) in a prompt string
 */
export function fillPrompt(
  template: string,
  variables: Record<string, string>
): string {
  let result = template;
  for (const [key, value] of Object.entries(variables)) {
    result = result.replace(new RegExp(`\\{\\{${key}\\}\\}`, "g"), value);
  }
  return result;
}
